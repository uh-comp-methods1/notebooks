{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19b9239-c063-4247-a9dc-3a05fe8fd61b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Linear equations: introduction\n",
    "\n",
    "This notebook is based on Chapter 2 of \n",
    "\n",
    "<a id=\"thebook\"></a>\n",
    "\n",
    "> Süli, Endre and Mayers, David F. _An introduction to numerical analysis_. Cambridge University Press, Cambridge, 2003.\n",
    "<https://doi.org/10.1017/CBO9780511801181> (ebook in [Helka](https://helka.helsinki.fi/permalink/358UOH_INST/1h3k2rg/alma9926836783506253))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facd944-8d42-40f5-ab55-de8aefcea63b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "Although numerical analysis has a long history, it can be argued, as is done [here](http://history.siam.org/), that \"modern\" numerical analysis, characterized by the synergy of the programmable electronic computer, mathematical analysis, and the opportunity and need to solve large and complex problems in applications, begins with the paper\n",
    "\n",
    "> von Neumann, John and Goldstine, Herman H. _Numerical inverting of matrices of high order_. Bulletin of the AMS 53, 1021-1099, 1947. <https://doi.org/10.1090/S0002-9904-1947-08909-6>\n",
    "\n",
    "Numerical inverting of matrices is the topic of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742954d-4d21-49be-9677-52e9e65ebaf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Determinants and computational cost\n",
    "\n",
    "We will consider linear equations \n",
    "$Ax = b$ \n",
    "where $A$ is an invertible $n \\times n$ matrix and $b$ is a vector in $\\mathbb R^n$,\n",
    "\n",
    "$$\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & \\dots & a_{1n}\n",
    "\\\\\n",
    "a_{21} & a_{22} & \\dots & a_{2n}\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "a_{n1} & a_{n2} & \\dots & a_{nn}\n",
    "\\end{pmatrix},\n",
    "\\qquad\n",
    "b = \n",
    "\\begin{pmatrix}\n",
    "b_1\n",
    "\\\\\n",
    "b_2\n",
    "\\\\\n",
    "\\vdots\n",
    "\\\\\n",
    "b_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Recall that $\\det(A)$ can be computed by expanding along row $i$ \n",
    "\n",
    "$$\n",
    "a_{i1} A_{i1} + a_{i2} A_{i2} + \\dots + a_{in} A_{in} = \\det(A),\n",
    "$$\n",
    "\n",
    "where $A_{ij} = (-1)^{i+j} M_{ij}$ and $M_{ij}$ is the determinant of the matrix obtained by deleting the row $i$ and column $j$ of $A$. Note that for $k \\ne i$\n",
    "\n",
    "$$\n",
    "a_{i1} A_{k1} + a_{i2} A_{k2} + \\dots + a_{in} A_{kn} = 0,\n",
    "$$\n",
    "\n",
    "since this is the determinant of the matrix where row $k$ is replaced by row $i$ in $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "292c1075-4dc3-4cf7-a603-8b090665ad83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To summarize,\n",
    "\n",
    "$$\n",
    "a_{i1} A_{k1} + a_{i2} A_{k2} + \\dots + a_{in} A_{kn} \n",
    "=\n",
    "\\begin{cases}\n",
    "\\det(A) & i = k,\n",
    "\\\\\n",
    "0 & i \\ne k.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We assumed that $A$ is invertible, equivalently $\\det(A) \\ne 0$. Hence\n",
    "\n",
    "$$ \n",
    "\\det(A)^{-1} (a_{i1} A_{k1} + a_{i2} A_{k2} + \\dots + a_{in} A_{kn})\n",
    "= \\delta_{ik}\n",
    "= \\begin{cases} \n",
    "1 & i = k,\n",
    "\\\\\n",
    "0 & i \\ne k.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Equivalently, $A A^{-1} = I$ with \n",
    "\n",
    "$$\n",
    "A^{-1} \n",
    "= \\frac{1}{\\det(A)} \n",
    "\\begin{pmatrix}\n",
    "A_{11} & A_{21} & \\dots & A_{n1}\n",
    "\\\\\n",
    "A_{12} & A_{22} & \\dots & A_{n2}\n",
    "\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "A_{1n} & A_{2n} & \\dots & A_{nn}\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a175057-3769-43ee-aa7a-6e3c2a994675",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This formula for the inverse $A^{-1}$ is called [Cramer's rule](https://en.wikipedia.org/wiki/Cramer's_rule). However, the formula does not give an efficient computational method. Let us consider the number $d_n$ of multiplication operations needed to compute $\\det(A)$ of a matrix of size $n \\times n$ using \n",
    "\n",
    "$$\n",
    "a_{i1} A_{i1} + a_{i2} A_{i2} + \\dots + a_{in} A_{in} = \\det(A).\n",
    "$$\n",
    "\n",
    "Then $d_n \\ge n d_{n-1}$ and $d_n \\ge n!$. The number $d_n$ is one measure of [computational complexity](https://en.wikipedia.org/wiki/Computational_complexity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2d3e7-9a62-4b79-8e04-5b63f186fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 100\n",
    "print(f'{np.math.factorial(n) = :.1e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1bfc27-ad0b-4b37-ab8d-275f8f24d536",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's find out if the world's fastest computer can handle $10^{156}$ floating-point operations (FLOPs)\n",
    "\n",
    "<https://www.wolframalpha.com/input/?i=fastest+computer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83679b-0bb3-4f80-8831-cc827d783900",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Peta](https://en.wikipedia.org/wiki/Peta-) is a decimal unit prefix denoting multiplication by $10^{15}$. So it would take at least $10^{156 - 15 - 3} = 10^{138}$ seconds to compute $\\det(A)$ for a matrix of size $100 \\times 100$. This is _much_ longer time than the age of the universe $4.3 \\cdot 10^{17}$ seconds, see\n",
    "\n",
    "<https://www.wolframalpha.com/input/?i=age+of+universe>\n",
    "\n",
    "To summarize, computations based on determinant expansions can be used only for very small matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd10217-5278-43e9-b242-2360eab0b4d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Gaussian elimination\n",
    "\n",
    "Gaussian elimination is one of the algorithms that are familiar to us. It consists of steps of the form \n",
    "\n",
    "1. Add a multiple of a row to another row\n",
    "2. Swap the position of two rows\n",
    "\n",
    "Let's solve $Ax = b$\n",
    "where \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    " 1  &\t 2  &\t 3  \t\\\\ \n",
    " 2  &\t 4  &\t 5  \t \\\\\n",
    " 3  &\t 5  &\t 6  \t \n",
    "\\end{pmatrix}, \\quad\n",
    "b = \\begin{pmatrix}\n",
    "1 \\\\ 0 \\\\ 0\n",
    "\\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffbda15b-f550-4d86-b888-606e0bedf6ca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Writing $a_1, a_2, a_3$ for the rows of $A$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "&\n",
    "\\begin{pmatrix}\n",
    " 1  &\t 2  &\t 3  &|&\t 1 \\\\\n",
    " 2  &\t 4  &\t 5  &|&\t 0 \\\\\n",
    " 3  &\t 5  &\t 6  &|&\t 0 \n",
    "\\end{pmatrix}\n",
    "\\xrightarrow{a_2 += -2 a_1}\n",
    "\\begin{pmatrix}\n",
    " 1  &\t 2  &\t 3  &|&\t 1 \\\\\n",
    " 0  &\t 0  &\t -1  &|&\t -2 \\\\\n",
    " 3  &\t 5  &\t 6  &|&\t 0 \n",
    "\\end{pmatrix}\n",
    "\\\\\n",
    "&\\xrightarrow{a_3 += -3 a_1}\n",
    "\\begin{pmatrix}\n",
    " 1  &\t 2  &\t 3  &|&\t 1 \\\\\n",
    " 0  &\t 0  &\t -1  &|&\t -2 \\\\\n",
    " 0  &\t -1  &\t -3  \t&|& -3 \n",
    "\\end{pmatrix}\n",
    "\\xrightarrow{a_2 \\leftrightarrow a_3}\n",
    "\\begin{pmatrix}\n",
    " 1  &\t 2  &\t 3  &|&\t 1 \\\\\n",
    " 0  &\t -1  &\t -3  &|&\t -3 \\\\\n",
    " 0  &\t 0  \t& -1  &|&\t -2 \n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Now $x$ can be solved by back substitution\n",
    "\n",
    "$$\n",
    "x_3 = 2, \\quad x_2 = 3 - 3x_3 = -3, \\quad x_1 = 1 - 2x_2 - 3x_3 = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e322de5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Matrices and vectors in NumPy\n",
    "\n",
    "Matrices and vectors are created using `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3617bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "[1,2,3],\n",
    "[2,4,5],\n",
    "[3,5,6],\n",
    "], dtype=float)\n",
    "b = np.array([1, 0, 0], dtype=float)\n",
    "print(f'{a = }')\n",
    "print(f'{b = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae48ac63-e72a-49b2-a126-95d65d16b0e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can go ahead and implement the Gaussian elimination in Python, and the below sketch can be used as a starting point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793ea66-eeb8-4c4e-9917-2e08a69ba856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from logging import debug, error\n",
    "\n",
    "def forward_solve(a, b):\n",
    "    '''Forward steps in Gaussian elimination. A sketch!'''\n",
    "    n = a.shape[0]\n",
    "    for j in range(n-1):\n",
    "        d = a[j,j]\n",
    "        if d == 0: # This is dangerous! Why?\n",
    "            error('Need to swap rows. This is not implemented!')\n",
    "            raise NotImplementedError()\n",
    "        for k in range(j+1,n):\n",
    "            mu = - a[k,j]/d\n",
    "            a[k] = a[k] + mu*a[j]\n",
    "            b[k] = b[k] + mu*b[j]\n",
    "        debug(f'After forward step for col {j+1} obtained:\\n'\n",
    "              f'{np.block([a,b[:,np.newaxis]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16a2df-e9cd-4a5d-9329-3d480513ea16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def backward_solve(a, b):\n",
    "    '''Backward steps in Gaussian elimination'''\n",
    "    n = a.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    for j in range(n-1,-1,-1):  \n",
    "        y = b[j]\n",
    "        for k in range(j+1, n):\n",
    "            y -= a[j,k]*x[k]        \n",
    "        x[j] = y/a[j,j]\n",
    "        debug(f'Solved x{j+1} = {x[j]}')\n",
    "    return x\n",
    "    \n",
    "def solve_demo(a, b):\n",
    "    '''Solve ax = b'''\n",
    "    # Take copies as we don't want to change the original matrices\n",
    "    a, b = a.copy(), b.copy() \n",
    "    forward_solve(a, b)\n",
    "    return backward_solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608027a-b587-4938-b227-14578833dd15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def test_solve(a, b):\n",
    "    x = solve_demo(a,b)\n",
    "    xtrue = np.linalg.solve(a, b)\n",
    "    print(f'{    x = }')\n",
    "    print(f'{xtrue = }')\n",
    "\n",
    "a = np.array([\n",
    "[1,2,3],\n",
    "[2,3,4],\n",
    "[3,4,6],\n",
    "], dtype=float)\n",
    "b = np.array([1, 1, 0], dtype=float)\n",
    "test_solve(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d903044-01fd-45bd-bfd0-7946ee3ad04e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try with the matrix from the example that we computed by hand\n",
    "a = np.array([\n",
    "[1,2,3],\n",
    "[2,4,5],\n",
    "[3,5,6],\n",
    "], dtype=float)\n",
    "b = np.array([1, 0, 0], dtype=float)\n",
    "try:\n",
    "    test_solve(a, b)\n",
    "except NotImplementedError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147ed77a-2018-4152-a489-6864291b29e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([\n",
    "[1, 0, 0, 0.1, 0],\n",
    "[0, 1, 0, 0.1, 0],\n",
    "[0, 0, 1, 0.1, 0],\n",
    "[1, 1, 1, 0.3, 1],\n",
    "[0, 0, 0, 1.0, 0]\n",
    "])\n",
    "b = np.array([0, 0, 0, 1, 1], dtype=float)\n",
    "logging.getLogger().setLevel(logging.WARNING);\n",
    "test_solve(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5292e163-a791-4a18-906c-2819343c7bb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We used `linalg.solve` of NumPy to compute the \"true\" solution. \n",
    "The documentation of this function tells that _solutions are computed using LAPACK routine gesv_. ([LAPACK](https://en.wikipedia.org/wiki/LAPACK) is a standard software library for numerical linear algebra.) The documentation of this routine can be found for example in Reference for Intel oneAPI Math Kernel Library, see [gesv](https://software.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/lapack-routines/lapack-linear-equation-routines/lapack-linear-equation-driver-routines/gesv.html#gesv). We find that _LU\n",
    "decomposition with partial pivoting and row interchanges is used_. \n",
    "\n",
    "Let's now study what this means. (Pivoting below means partial pivoting in the above sense of row interchanges, see the Wikipedia article [Pivot element](https://en.wikipedia.org/wiki/Pivot_element#Partial_and_complete_pivoting) for description of complete pivoting.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b32eb1-355e-47da-a717-82a075867b31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LU factorization\n",
    "\n",
    "Let us express the operation\n",
    "\n",
    "1. Add a multiple of a row to another row\n",
    "\n",
    "by using matrix multiplication. Let $E^{(rs)}$ be the matrix whose only nonzero element is 1 at row $r$, column $s$. Let $A$ be a matrix with elements $a_{ij}$. Then\n",
    "\n",
    "$$\n",
    "(E^{(rs)} A)_{ij} = \\sum_{k=1}^n (E^{(rs)})_{ik} a_{kj}\n",
    "= \\sum_{k=1}^n \\delta_{ri}\\delta_{sk} a_{kj}\n",
    "= \\delta_{ri} a_{sj}\n",
    "$$\n",
    "\n",
    "This is the matrix whose row $r$ is the row $s$ of $A$, and all the other rows are zero. It follows that $(I + \\mu E^{(rs)}) A$ is the matrix obtained from $A$ by adding $\\mu$ times its row $s$ to its row $r$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b07d9fe-b26c-4492-bfae-35b8a37f68c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we don't need to swap rows in Gaussian elimination, then the algorithm gives an upper triangular matrix $U$ satisfying\n",
    "\n",
    "$$\n",
    "L_{(N)} L_{(N-1)} \\dots L_{(1)} A = U\n",
    "$$\n",
    "\n",
    "where each $L_{(j)}$ is a matrix of the form $(I + \\mu E^{(rs)}) A$ with some $\\mu$ and $r > s$. \n",
    "\n",
    "There holds $(I + \\mu E^{(rs)})^{-1} = (I - \\mu E^{(rs)})$, since this undoes the addition. In particular,\n",
    "\n",
    "$$\n",
    "A = LU, \\qquad L = L_{(1)}^{-1} \\dots L_{(N-1)}^{-1} L_{(N)}^{-1}.\n",
    "$$\n",
    "\n",
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Lemma: product of lower triangular matrices\n",
    "\n",
    "The product of two lower triangular matrices is lower triangular. \n",
    "If the diagonal elements of both the factors are all equal to one, the same holds for product. \n",
    "In particular, $L$ is lower triangular with all its diagonal elements equal to one.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d5dc6-c837-4ea0-b47c-25d653568b24",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "_Proof._ Consider two lower triangular matrices $A$ and $B$ with elements $a_{ij}$ and $b_{ij}$. Then $a_{ij} = b_{ij} = 0$ if $j > i$, and\n",
    "\n",
    "$$\n",
    "(AB)_{ij} = \\sum_{k=1}^n a_{ik} b_{kj} = \\sum_{k=j}^i a_{ik} b_{kj}.\n",
    "$$\n",
    "\n",
    "In particular, $(AB)_{ij} = 0$ if $j > i$ since in this case we are summing over an empty set of indices.\n",
    "\n",
    "Rest of the proof is left as an exercise. \n",
    "$\\blacksquare$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2842c6c9-7b2b-41d2-9458-75681b046383",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Pivoting\n",
    "\n",
    "Let us now turn to the pivoting operation \n",
    "\n",
    "2. Swap the position of two rows\n",
    "\n",
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Definition: permutation matrix\n",
    "\n",
    "A square matrix is a permutation matrix if each of its elements is either one or zero and if each of its rows and colums contain exactly one nonzero element.\n",
    "</div>\n",
    "\n",
    "Let $P^{(rs)}$ be the permutation matrix obtained by swapping rows $r$ and $s$ of the identity matrix. Then $P^{(rs)} A$ is the matrix obtained by swapping rows $r$ and $s$ of $A$. (For the short proof see e.g. Lemma 2.5.13 of Pekka Pankka's notes on linear algebra.) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c35d81e5-7a1e-42d1-8897-8d454980fcee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Lemma: product of permutation matrices\n",
    "\n",
    "The product of two permutation matrices is a permutation matrix.\n",
    "</div>\n",
    "\n",
    "  \n",
    "\n",
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Lemma: inverse of a permutation matrix\n",
    "\n",
    "If $P$ is a permutation matrix then $P^{-1}$ exists.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef67d08-7708-4c3b-afcf-edd99d947684",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "_Proof of the product of permutation matrices lemma._\n",
    "Let $P \\in \\mathbb R^{n \\times n}$ be permutation matrix and write $\\tau(j)$ for the row index of the unique nonzero element in column $j$. As there is exactly one nonzero element on each row, we see that $\\tau$ is a permutation of the set $\\{1,\\dots,n\\}$. Write $e_1,\\dots,e_n$ for the standard basis of $\\mathbb R^n$. Then $P$ has the columns $e_{\\tau(1)}, \\dots, e_{\\tau(n)}$ and so $P e_j = e_{\\tau(j)}$. \n",
    "\n",
    "Let $Q \\in \\mathbb R^{n \\times n}$ be another permutation matrix and write $\\sigma$ for the corresponding permutation. Then \n",
    "$$\n",
    "Q P e_j = Q e_{\\tau(j)} = e_{\\sigma(\\tau(j))}.\n",
    "$$\n",
    "It follows that $QP$ is the permutation matrix corresponding to the permutation $\\sigma \\circ \\tau$.\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879da50-eeee-4d24-bf0c-32bcb3f8ef44",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "Using the language of group theory, $n \\times n$ permutation matrices give a faithful representation of the group of permutations of a set with $n$ elements. \n",
    "\n",
    "Proof of the _inverse of a permutation matrix_ lemma is left as an exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c71d54ff-4d1f-40bf-aa9b-0b0f23692dd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Theorem: LU factorization with pivoting\n",
    "\n",
    "For a square matrix $A$ there are lower and upper triangular $L$ and $U$ and a permutation matrix $P$ such that $PA = LU$. \n",
    "</div>\n",
    "\n",
    "For a proof, see Theorem 2.3 in [the book](#thebook). From the computational point of view, a good pivoting strategy is to swap rows so that `d` in `forward_solve` is as large as possible. The proof of the theorem reflects this choice. \n",
    "\n",
    "Let's see how the factorization can be used when solving $A x = b$.\n",
    "As $P$ and $L$ are always invertible, $A$ is invertible if and only if $U$ is. Suppose that this is the case. We write $y = U x$. Then $A x = b$ is equivalent with $L y = P b$. As $L$ is triangular, $y$ can be solved easily. Then $x$ can be solved easily form $y = U x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5db3b0-90aa-4ad4-b52e-bcb4e9d741e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LU factorization is implemented in SciPy\n",
    "from scipy import linalg as la\n",
    "a = np.array([\n",
    "[1, 0, 0, 0.1, 0],\n",
    "[0, 1, 0, 0.1, 0],\n",
    "[0, 0, 1, 0.1, 0],\n",
    "[1, 1, 1, 0.3, 1],\n",
    "[0, 0, 0, 1.0, 0]\n",
    "])\n",
    "b1 = np.array([0, 0, 0, 1, 1], dtype=float)\n",
    "b2 = np.array([1, 1, 1, 0, 0], dtype=float)\n",
    "# As the diagonal of L consists of ones, SciPy does not store it\n",
    "# Both L and U are stored in the same matrix\n",
    "# The permutation matrix is represented by the pivot indices:\n",
    "# row i of matrix was interchanged with row piv[i]\n",
    "lu, piv = la.lu_factor(a)\n",
    "# Once we have the LU factorization, we can solve Ax = b\n",
    "# for several right-hand sides very quickly\n",
    "print(f'{la.lu_solve((lu, piv), b1) = }')\n",
    "print(f'{la.lu_solve((lu, piv), b2) = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ebb575-d387-4a93-93ad-539b28540573",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fd28ff-0947-4fba-82bb-e2b46d5b326e",
   "metadata": {},
   "source": [
    "The norm of a vector $v \\in \\mathbb R^n$ with elements $v_1, \\dots, v_n$ is\n",
    "\n",
    "$$\n",
    "|v| = \\sqrt{|v_1|^2 + \\dots + |v_n|^2}\n",
    "$$\n",
    "\n",
    "and the norm of $A \\in \\mathbb R^{n \\times n}$ is\n",
    "\n",
    "$$\n",
    "|A| = \\max_{v \\in \\mathbb R^n \\setminus \\{0\\}} \\frac{|Av|}{|v|}.\n",
    "$$\n",
    "\n",
    "If $A$ is invertible, then its _condition number_ is \n",
    "\n",
    "$$\n",
    "\\kappa(A) = |A| |A^{-1}|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c496d30-013b-419a-9d06-e98cadda8726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the equation $Ax = b$ and suppose that $b$ is known only up to an error $e$.\n",
    "That is, we have the data $\\tilde b = b + e$.\n",
    "Then, instead of finding $x = A^{-1} b$, the best that we can do is to compute $\\tilde x = A^{-1} \\tilde b$. \n",
    "\n",
    "The ratio of the relative error in the solution to the relative error in the data is\n",
    "\n",
    "$$\n",
    "\\frac{|\\tilde x - x|/|x|}{|\\tilde b - b|/|b|}\n",
    "= \n",
    "\\frac{|A^{-1} e|/|x|}{|e|/|b|}\n",
    "=\n",
    "\\frac{|A^{-1} e|}{|e|} \\frac{|A x|}{|x|}.\n",
    "$$\n",
    "\n",
    "Maximizing this ratio in both $e \\ne 0$ and $x \\ne 0$ gives $\\kappa(A)$. \n",
    "The condition number measures how much the solution $x$ to $Ax = b$ can change for a small change in the data $b$. If $\\kappa(A)$ is large then $Ax = b$ is hard to solve computationally. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87c58ae2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Lemma: norm of a product\n",
    "\n",
    "For $A, B \\in \\mathbb R^{n \\times n}$\n",
    "\n",
    "$$\n",
    "|AB| \\le |A|\\, |B|\n",
    "$$\n",
    "\n",
    "In particular, $\\kappa(A) \\ge 1$ for invertible $A$.\n",
    "</div>\n",
    "\n",
    "For a proof, see Theorem 2.10 in [the book](#thebook). \n",
    "\n",
    "The square roots of the eigenvalues of $A^{\\mathsf T} A$ are called the _singular values_ of $A$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ca8705b-bbd2-4aaa-9c7c-9bcee110a312",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"padding:25px; border: 2px solid gray;\">\n",
    "\n",
    "## Lemma: condition number and singular values\n",
    "\n",
    "The norm $|A|$ is equal to the largest singular value of $A$. \n",
    "For invertible $A$, the condition number $\\kappa(A)$ is the ratio of the largest singular value of $A$ to the smallest one.\n",
    "\n",
    "If $A \\in \\mathbb R^{n \\times n}$ is symmetric and positive definite (that is, $A^{\\mathsf T} = A$ and $x^{\\mathsf T} A x > 0$ for all nonzero $x \\in \\mathbb R^n$), \n",
    "then the condition number $\\kappa(A)$ is the ratio of the largest eigenvalue of $A$ to the smallest one.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45c7e444-adca-4eb1-8fe3-ca404ed684ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_Proof_. See Theorem 2.9 in [the book](#thebook) for a proof of the claim on $|A|$.\n",
    "As in that proof, let $w_i$, $i=1,\\dots,n$, be an orthonormal basis of eigenvectors of $A^{\\mathsf T} A$ corresponding to eigenvalues $\\lambda_i$, given in increasing order. Then for a vector $u$ given in the eigenbasis\n",
    "\n",
    "$$\n",
    "u = c_1 w_1 + \\dots + c_n w_n\n",
    "$$\n",
    "\n",
    "there holds\n",
    "\n",
    "$$\n",
    "|u|^2 = c_1^2 + \\dots + c_n^2, \n",
    "\\qquad\n",
    "|Au|^2 = c_1^2 \\lambda_1 + \\dots + c_n^2 0 \\lambda_n \\ge \\lambda_1 |u|^2, \n",
    "$$\n",
    "\n",
    "with equality when $u = w_1$. Suppose that $v \\in \\mathbb R^n$ is nonzero and set $u = A^{-1} v$. Then\n",
    "\n",
    "$$\n",
    "|A^{-1} v| = |u| \\le \\lambda_1^{-1/2} |Au| = \\lambda_1^{-1/2} |v|\n",
    "$$\n",
    "\n",
    "and $|A^{-1}| \\le \\lambda_1^{-1/2}$. Equality is achieved with $v = A w_1$.\n",
    "Hence $|A^{-1}| = \\lambda_1^{-1/2}$ and the claim follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c2d04-033a-4f70-a67f-d423bbb9bdaf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The claim on the positive definite case follows from the fact that $\\lambda_i = \\mu_i^2$ for the eigenvalues $\\mu_i$ of $A$. To prove this equality we write $w_i$ for a basis of eigenvectors of $A$ (the computation below shows that that this is also a basis of eigenvectors of $A^{\\mathsf T} A$, so there is no conflict in notation). Then, using $A^{\\mathsf T} = A$,\n",
    "\n",
    "$$\n",
    "A^T A w_i = A^2 w_i = A (\\mu_i w_i) = \\mu_i^2 w_i.\n",
    "$$\n",
    "$\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5eedef-7950-4c8a-919b-b3804c289430",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Hilbert matrix\n",
    "\n",
    "The Hilbert matrix $H_{(n)} \\in \\mathbb R^{n \\times n}$ has the elements\n",
    "\n",
    "$$\n",
    "h_{ij} = \\frac{1}{i + j - 1}\n",
    "$$\n",
    "\n",
    "Let's find the condition number $\\kappa(H_{(n)})$ for some small values of $n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ea71c-b9ef-43b4-9adf-eb842be8132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def element(i0, j0):\n",
    "    i, j = i0 + 1, j0 + 1\n",
    "    return 1 / (i + j - 1)\n",
    "\n",
    "def hilbert(n):\n",
    "    '''Construct the Hilbert matrix of size n x n'''\n",
    "    return np.fromfunction(element, (n, n))\n",
    "\n",
    "hilbert(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc13b0-429b-4001-b9b4-28dc429fb767",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def kappa_demo(a):\n",
    "    '''Compute the largest and smallest eigenvalues \n",
    "       and the condition number of a positive definite matrix'''\n",
    "    ls = np.linalg.eigvals(a)\n",
    "    lmax, lmin = np.max(ls), np.min(ls)\n",
    "    kappa = lmax/lmin\n",
    "    return lmax, lmin, kappa\n",
    "\n",
    "ns = range(2,13,2)\n",
    "data = [kappa_demo(hilbert(n)) for n in ns]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337f243-6215-4726-b218-81f72c4ea976",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use pandas to display the data prettily\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['$\\lambda_{max}$', '$\\lambda_{min}$', '$\\kappa$']\n",
    "df.index = ns\n",
    "df.index.name = 'n'\n",
    "df.style.format('{:.1e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9ebb54-cf69-4667-913e-a03dbc7f0a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We can also compute the condition number directly using NumPy \n",
    "kappas = [np.linalg.cond(hilbert(n)) for n in ns]\n",
    "df = pd.DataFrame({\n",
    "    '$\\kappa$ demo': np.array(data)[:,2], \n",
    "    '$\\kappa$ NumPy': kappas, \n",
    "    })\n",
    "df.index = ns\n",
    "df.index.name = 'n'\n",
    "df.style.format('{:.1e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b45263-970c-4966-bdcc-badf3d7c0fab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the problem to approximate a function $f$ by a polynomial $p$ of degree $n$ so that the following distance, that is, the distance in $L^2(0,1)$, is minimized\n",
    "\n",
    "$$\n",
    "\\|f - p\\|_{L^2(0,1)} = \\left(\\int_0^1 |f(x) - p(x)|^2 dx \\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "We write $p(x) = c_0 + c_1 x + \\dots + c_n x^n$ and note that the gradient of\n",
    "\n",
    "$$\n",
    "\\|f - p\\|_{L^2(0,1)}^2 = \\|f\\|_{L^2(0,1)}^2 - 2 \\sum_{i = 0}^n c_i \\int_0^1 f(x) x^i dx + \\sum_{i,j=0}^n c_i c_j \\int_0^1 x^{i + j} dx,\n",
    "$$\n",
    "\n",
    "with respect to $c = (c_0, \\dots, c_n) \\in \\mathbb R^{n+1}$, must vanish when the distance is minimized. This leads to the linear system $M c = b$ \n",
    "where\n",
    "\n",
    "$$\n",
    "M_{ij} = \\int_0^1 x^{i + j} dx = \\frac{1}{i + j + 1},\n",
    "\\quad\n",
    "b_i = \\int_0^1 f(x) x^i dx, \\qquad i,j=0,\\dots,n.\n",
    "$$\n",
    "\n",
    "Here $M$ is the Hilbert matrix $H_{(n+1)}$, with the elements indexed starting from zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bcd808",
   "metadata": {},
   "source": [
    "This implies that the usual polynomial basis $\\lbrace 1, x, x^2, ..., x^{n-1} \\rbrace$ may not be the best option in numerical applications. In [04-interpolation](../04-interpolation/lecture.ipynb) we will cover numerically stable options for polynomial interpolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf1052-18da-49a4-a2a5-88e3cf0056cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Least squares method\n",
    "\n",
    "Linear _least squares problem_ is of the form:\n",
    "\n",
    "> Given a matrix $A \\in \\mathbb R^{m \\times n}$ and a vector $y \\in \\mathbb R^m$, find a vector $x \\in \\mathbb R^n$ \n",
    "that minimizes the squared distance $|Ax - y|^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2705dbc",
   "metadata": {},
   "source": [
    "Write $f(x) = |Ax - y|^2$. If $f$ achieves its minimum at $x$ then $\\nabla f(x) = 0$.\n",
    "This is equivalent with the _normal equation_\n",
    "\n",
    "\\begin{align}\\tag{1}\n",
    "A^{\\mathsf T} A x = A^{\\mathsf T} y.\n",
    "\\end{align}\n",
    "\n",
    "We can solve this system by usign the LU factorization, but this is not the best option. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df9cf3",
   "metadata": {},
   "source": [
    "### Definite matrix\n",
    "\n",
    "A symmetric matrix (i.e. $A = A^T \\in \\mathbb{R}^{n \\times n}$ and analogously for $\\mathbb{C}^{n \\times n}$) is _positive definite_ if $\\forall x \\in \\mathbb{R}^{n}, x \\neq 0$:\n",
    "$$\\ x^T A x > 0.$$\n",
    "\n",
    "Positive definiteness is a strong property with many helpful properties. It also turns out that if $A \\in \\mathbb{R}^{m \\times n}$ is a full row rank matrix ($\\text{rank}(A) = n$), then the matrix $A^T A$ is symmetric positive definite (spd)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33afa6ba-5802-4a69-9b24-3e12783e1b1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: condition number of $A^{\\mathsf T} A$\n",
    "\n",
    "Consider the matrix\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "\\epsilon & 0\n",
    "\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "for small $\\epsilon > 0$, then $\\kappa(A) = \\epsilon^{-1}$ and $\\kappa(A^{\\mathsf T} A) = \\epsilon^{-2}$. Hence inverting $A^{\\mathsf T} A$ can be much harder than inverting $A$. A better way to solve the least squares problem is to use the [QR factorization](https://en.wikipedia.org/wiki/QR_decomposition) (see Theorem 2.12 in [the book](#thebook)) or [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition). \n",
    "In practice, we can use\n",
    "[lstsq](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) of SciPy that \n",
    "uses QR factorization with `lapack_driver='gelsy'` and singular value decomposition with `lapack_driver='gelsd'` (the default option). \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc57c303-7614-4db8-ac3f-c9e15ca51496",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: linear regression \n",
    "\n",
    "Given points $(x_j,y_j)$, $j=1,2,\\dots,N$,\n",
    "that are\n",
    "generated by a process obeying the equation for a line\n",
    "$y=kx + b$, but that are corrupted by noise.\n",
    "Find $k$ and $b$. The parameters $k$ and $b$ match with the data \n",
    "$(x_j,y_j)$, $j=1,2,\\dots,N$, in the best possible way when they minimize the squared distances\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sum_{j=1}^N |kx_j + b - y_j|^2.\n",
    "    \\end{equation*}\n",
    "\n",
    "But this sum coincides with $|Ax -y|^2$\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "A = \n",
    "\\begin{pmatrix}\n",
    "1 & x_1\n",
    "\\\\\n",
    "1 & x_2\n",
    "\\\\\\vdots & \\vdots\\\\\n",
    "1 & x_N\n",
    "\\end{pmatrix}, \\quad\n",
    "x = \n",
    "\\begin{pmatrix}\n",
    "b \\\\ k\n",
    "\\end{pmatrix}, \\quad\n",
    "y = \n",
    "\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\\\vdots\\\\ y_N\n",
    "\\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "    \n",
    "We arrive to the least squares problem to minimize $|Ax-y|^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aacc3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import linalg as la\n",
    "rng = np.random.default_rng()\n",
    "def generate_data(N):\n",
    "    k = 1.1\n",
    "    b = 0.2\n",
    "    xs = rng.uniform(0, 1, size=N)\n",
    "    noise = rng.normal(scale=0.1, size=N)\n",
    "    ys = k*xs + b + noise\n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = generate_data(100)\n",
    "\n",
    "a = np.ones((np.size(xs), 2))\n",
    "a[:, 1] = xs\n",
    "x, _, _, _ = la.lstsq(a, ys)\n",
    "b, k = x\n",
    "print(f'{b = :.2f}, {k = :.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8738f-3afe-4026-9143-088b4bd89265",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(xs, ys)\n",
    "xs_plot = np.linspace(0,1)\n",
    "plt.plot(xs_plot, k*xs_plot + b, 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: plane least squares\n",
    "Toy example of fitting a plane into 3D points.\n",
    "\n",
    "The equation of a plane is given by $a x + by + c = z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0_error = 0.0 # Change me\n",
    "\n",
    "points = np.array([[1, p0_error, 0], \n",
    "                   [0.01, 1, 0], \n",
    "                   [0, 0, 2], \n",
    "                   [1/(1.41), 1/(1.41), 0], \n",
    "                   [0.3, 0.5, 0.59], \n",
    "                   [0.2, 0.2, 1.2], \n",
    "                   [0.66, 0.75, -0.37]])\n",
    "m, d = points.shape\n",
    "A = np.hstack([points[:,0:2], np.ones((m, 1))])\n",
    "y = points[:,d-1]\n",
    "\n",
    "plane, _, _, _ = la.lstsq(A, y)\n",
    "a, b, c = plane\n",
    "print(f'Equation of the plane: z = {a:.2f}x + {b:.2f}y + {c:.2f}')\n",
    "\n",
    "n = 10\n",
    "xx, yy = np.meshgrid(np.linspace(0, 1, n), np.linspace(0, 1, n))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.plot_surface(xx, yy, a*xx + b*yy + c, alpha=0.2, rstride=2, cstride=2)\n",
    "ax.scatter(points[:,0], points[:,1], points[:,2], 'o', color='red')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.view_init(elev=15, azim=-45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ab76f",
   "metadata": {},
   "source": [
    "## Example: Coope's method for circle fitting\n",
    "\n",
    "Consider sequence of points $y_j \\in \\mathbb{R}^n, \\ j = 1,2,...,N$ which we expect to lay on a ($n$-dimensional) circle with radius $r > 0$ and center $x \\in \\mathbb{R}^n$. If we wish to find $r$ and $x$ a sensible start would be to try minimize\n",
    "$$ \\sum_{j = 1}^N | \\| x - y_j \\| - r |^2,$$\n",
    "but as shown in\n",
    "> Dooke, Ian. *Coope, Ian D. \"Circle fitting by linear and nonlinear least squares*. Journal of Optimization theory and applications, 76 (1993): 381-388. [link to paper](https://core.ac.uk/download/pdf/35472611.pdf).\n",
    "\n",
    "This is a nonlinear problem and sensitive to outliers.\n",
    "\n",
    "Coope proposed a linearized variant which is more robust: goal is to find $r, x$ which minimize\n",
    "\n",
    "$$ \\sum_{j = 1}^N | f_j(x,r) |^2,$$\n",
    "where $f_j(x,r) = \\| x - y_j \\|^2 - r^2 = \\| x \\|^2 - 2 \\langle x, y_j \\rangle + \\| y_j \\|^2 - r^2$. Denoting\n",
    "$$ A = \\begin{pmatrix} \n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "y_1 & y_2 & \\dots & y_N \\\\\n",
    "\\vdots & \\vdots & & \\vdots \\\\\n",
    "1 & 1 & \\dots & 1\n",
    "\\end{pmatrix}, \\ z = \\begin{pmatrix}\n",
    "\\vdots \\\\\n",
    "2x \\\\\n",
    "\\vdots \\\\\n",
    "r^2 - \\| x \\|^2\n",
    "\\end{pmatrix} \\ \\text{and} \\ d = \\begin{pmatrix}\n",
    "\\|y_1\\|_2^2 \\\\\n",
    "\\|y_2\\|_2^2 \\\\\n",
    "\\vdots \\\\\n",
    "\\|y_N\\|_2^2\n",
    "\\end{pmatrix}.$$\n",
    "We get an equivalent linear problem\n",
    "$$ \\min_{z} \\| Az - d \\|_2^2,$$\n",
    "where the coordinates of the center point are $x_i = \\frac{1}{2}z_i, \\ i = 1,2,...,n$ and the radius $r = \\sqrt{z_{n+1} - \\| x \\|^2}$. \n",
    "\n",
    "There are also many other methods for fitting circle and other shapes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Random sample consensus (RANSAC)\n",
    "\n",
    "[RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus) is an iterative parameter fitting method for data which contain outliers which would make a simple least-squares fit unusable. It was introduced in \n",
    "> Fischler, Martin and Bolles, Robert. *Random samples consensus: a pradigm for model fitting with applications to image analysis and autoamted cartography*. Communications of the ACM, 24(6):381–395, 1981. [DOI:10.1145/358669.358692](https://doi.org/10.1145/358669.358692)\n",
    "\n",
    "In summary (with an affine model):\n",
    "\n",
    "0. Start by picking a random subset of data points $y^{(0)}$. The size of the subset should be chosen as small as reasonably possible.\n",
    "1. Given a subset of the data points $y^{(k)}$ at iteration $k$, fit a model $(A^{(k)} x^{(k)} = y^{(k)})$ to that smaller problem using least squares.\n",
    "2. The quality of the model is then assessed on the whole dataset by checking how many points lie within a given *confidence parameter* of the candidate model. These points are called the *consensus set*.\n",
    "3. If the consensus set is sufficiently big, use it as the next subset of data points $y^{(k+1)}$. Otherwise, randomly pick a new subset of data points $y^{(k+1)}$.\n",
    "4. Terminate if $k = K_{\\max}$ or some other stopping criterion is met. Otherwise increment $k$ and go to step 1.\n",
    "\n",
    "RANSAC is used in computer vision applications where the measured data $y$ is a noisy and possibly erronous [point cloud](https://en.wikipedia.org/wiki/Point_cloud) in 3D space and we could, for example, try to find a plane or a vertex of a polygon that best matches most of the individual points.\n",
    "\n",
    "Multiple point clouds from different directions can also be matched using RANSAC to estimate the transformation $T$ which determines the change in measurement directions. This is called [point-set registration](https://en.wikipedia.org/wiki/Point-set_registration)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b05918e-ca60-42f7-90f2-b3672910a6ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Further reading\n",
    "\n",
    "Numerical linear algebra is a vast subject. Chapters 3 and 5 of [the book](#thebook) contain important topics that we won't cover. If you want to learn more about these topics, you can exercise your [flexible study right](https://studies.helsinki.fi/instructions/article/flexible-study-right-joo) and take the numerical matrix computations course at Aalto.\n",
    "\n",
    "For an overview of linear algebra capabilities of SciPy see the [tutorial](https://docs.scipy.org/doc/scipy/tutorial/linalg.html). (The tutorial mentions the [outdated](https://numpy.org/devdocs/reference/generated/numpy.matrix.html) matrix class of NumPy, and you can just skip the section on this.) \n",
    "\n",
    "Methods to solve unstable and ill-posed problems are covered in the [inverse problems](https://studies.helsinki.fi/courses/course-unit/otm-c8ec19f4-5057-4ba3-b283-b53867483b09/MAST31401) courses (taught in periods 1 and 2). In particular the [singular value decomposition (SVD)](https://en.wikipedia.org/wiki/Singular_value_decomposition) is covered and used extensibly so we will not cover it in further detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4a5d8-3b7d-48e7-86f2-93c821b87742",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "c8756df47763816d6300c4662f6feb03c22f6141c683808b625b74659b62406d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
