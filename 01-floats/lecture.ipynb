{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b31263-6b6d-4c50-9999-8b9ffa84ebb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "# Floating-point numbers: introduction\n",
    "\n",
    "Equality is tested using `==` and `=` is reserved for variable substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f900463-a068-4345-a626-a127bb0303f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = 1 # variable a gets value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f8633-919e-4394-add6-7b99b8314f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "a == 1 # check if a is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c7b663-75d1-45a3-9ede-ce52afe40f6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "source": [
    "What does the following cell output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249123ac-5f3d-4548-bc25-f0efafbb8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1 + 0.1 + 0.1 == 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0249509-bc7e-41b6-ba71-8846e8958654",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This notebook explains what happens here, but in short there are two things happening: computers store numbers in binary, but a finite decimal expnasion can be infinite in binary; and computers only store finitely many values (leading to truncation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d7005-4e02-4b8b-933b-62bc036817dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "slide"
    ]
   },
   "source": [
    "# Binary numbers\n",
    "\n",
    "We are used to the decimal positional system\n",
    "$$\n",
    "\\pi = 3.14159\\dots\n",
    "$$\n",
    "\n",
    "Some rational numbers have infinite decimal expansion\n",
    "\n",
    "$$\n",
    "\\frac 1 3 = 0.333\\dots\n",
    "$$\n",
    "Computers use the binary positional system\n",
    "\n",
    "$$\n",
    "1 \\equiv 1, \\quad 2 \\equiv 10, \\quad 3 \\equiv 11, \\quad 4 \\equiv 100, \\quad \\dots\n",
    "$$\n",
    "\n",
    "The integers are fine: any integer sequence in base-10 corresponds to a finite (though longer) sequence in binary and vice versa.\n",
    "\n",
    "However, some rational numbers have finite decimal expansion but infinite binary expansion\n",
    "\n",
    "$$\n",
    "\\frac 1 {10} \\equiv 0.00011{\\bf 0011}00110011\\dots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b1897-aa75-4f57-acce-66cfa8033c1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "subslide"
    ]
   },
   "source": [
    "## Long division\n",
    "\n",
    "Recall the [long division](https://en.wikipedia.org/wiki/Long_division) algorithm that you have learned in elementary school. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56468d36-be29-4acd-87bb-d18a52e290c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def expand_ratio(x, y, max_steps = 10, base = 10):\n",
    "    '''Expand x/y in a base (by default in the decimal base)'''\n",
    "    q, r = divmod(x, y)\n",
    "    output = np.base_repr(q, base)\n",
    "    output += '.'\n",
    "    for n in range(max_steps):\n",
    "        if r == 0:\n",
    "            break\n",
    "        r = base*r\n",
    "        q, r = divmod(r, y)\n",
    "        output += np.base_repr(q, base)\n",
    "    return output\n",
    "    \n",
    "print(expand_ratio(1, 10, base = 10))\n",
    "print(expand_ratio(1, 10, base = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a63ab2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IEEE 754 floating-point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e6b1be",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This section is inspired by [Section 15](https://docs.python.org/3/tutorial/floatingpoint.html#floating-point-arithmetic-issues-and-limitations) of the excellent [Python Tutorial](https://docs.python.org/3/tutorial/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edb00e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Infinite expansions of numbers need to be truncated in computations:\n",
    "\n",
    "Numbers are stored using 53 bits (that is, binary digits) of precision (this part of the float is called the _mantissa_)\n",
    "  * $2^0 = 1 \\equiv 1$ can be stored using 1 bit, \n",
    "  * $2^1 = 2 \\equiv 10$ and $3 \\equiv 11$  can be stored using 2 bits, \n",
    "  * $2^2 = 4 \\equiv 100, 5, 6$ and $7$  can be stored using 3 bits and so on. \n",
    "\n",
    "53 bits can store any integer between 0 and $2^{54} - 1$. Notice that even if we store the number 4, all bits are still reserved in the memory. So if large numbers are not needed, less bits should be reserved for each number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Largest integer stored with 53 bits: {2**54 - 1}')\n",
    "print(f'Largest 8-bit unsigned (positive only) integer, `uint8`: {2**8 - 1}')\n",
    "print(f'Largest 16-bit signed integer, `int16`: {2**15 - 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e396fa1",
   "metadata": {},
   "source": [
    "The idea of floating-point numbers or floats is the same as [scientific notation](https://en.wikipedia.org/wiki/Scientific_notation): we separate the decimal part or significant figures and the (exponent of the) magnitude to get a logarithmic spacing of values covering both very small and very large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd913d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "ax = plt.axes(xscale='log', yscale='linear', xlabel='Logarithmic scale', ylabel='Linear scale')\n",
    "ax.grid()\n",
    "ax.set_xlim((10**-5,10**3))\n",
    "ax.set_ylim((0,10**3))\n",
    "ax.yaxis.set_minor_locator(plt.MaxNLocator(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132560e8",
   "metadata": {},
   "source": [
    "\n",
    "* Python uses [IEEE 754 floating-point](https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64) arithmetic, where every number is stored using 64 bits (this is also known as _double precision_, compared to _single precision_ which uses 32 bits, _half precision_ which uses 16 bits etc.). \n",
    "* One bit is reserved for the sign: $0 \\sim$ positive, $1 \\sim$ negative.\n",
    "* 11 bits are used for the exponent\n",
    "* 53 bits are used for the mantissa\n",
    "\n",
    "\n",
    "\n",
    "On input 0.1 is converted to the closest number of the form $J 2^{-N}$ where \n",
    "$J$ and $N$ are integers that are normalized so that $2^{52} \\le J < 2^{53}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903faa3-f881-45c2-9b0a-9dcab2bc727d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $x = 0.1$ and write\n",
    "$x \\approx J 2^{-N}$\n",
    "for the best approximation with $J$ satisfying\n",
    "\n",
    "$$\n",
    "2^{52} \\le J < 2^{53}.\n",
    "$$\n",
    "\n",
    "We replace $J$ by $x 2^N$ and take base 2 logarithm, denoted by $\\log_2$,\n",
    "\n",
    "$$\n",
    "52 \\le \\log_2(x) + N < 53.\n",
    "$$\n",
    "\n",
    "This implies that $N$ must be chosen as the smallest integer satisfying \n",
    "\n",
    "$$\n",
    "52 - \\log_2(x) \\le N.\n",
    "$$\n",
    "\n",
    "Equivalently, $N = 52 - n$ where $n$ is the largest integer satisfying\n",
    "\n",
    "$$\n",
    "n \\le \\log_2(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a08d8b5-62fd-43ad-a15f-bebcff6f6e0b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Let's now solve for N and J\n",
    "x = 0.1\n",
    "n = np.floor(np.log2(x))\n",
    "N = 52 - n\n",
    "J = np.round(x * 2**N)\n",
    "print(f'{N = }, {J = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8eb1a-8c0f-4a82-9f6d-d2231db1943c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Note that J is even so that we can also choose \n",
    "N = N - 1\n",
    "J = J / 2\n",
    "print(f'{N = }, {J = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65be36f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The above normalization explains what _\"Significand precision: 53 bits (52 explicitly stored)\"_ means in the [IEEE 754 floating-point](https://en.wikipedia.org/wiki/Double-precision_floating-point_format#IEEE_754_double-precision_binary_floating-point_format:_binary64) article in Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cb683-e2cf-4f39-8373-cb4a8fde8d8c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To summarize, when we input 0.1, what the computer actually sees is \n",
    "\n",
    "$$\n",
    "J 2^{-N} = 3602879701896397 \\cdot 2^{-55},\n",
    "$$\n",
    "\n",
    "stored in binary. This number is not exactly 0.1. (It can not be since 0.1 has infinite expansion in binary.) \n",
    "\n",
    "To see the difference, we compare $J$ to\n",
    "$$\n",
    "x 2^N = 0.1 \\cdot 2^N = \\frac{2^N}{10}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45733fc5-6a3a-4159-9f62-a7714b5f1f30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# J = 2^N/10 up to a small remainder\n",
    "q, r = divmod(2**N, 10)\n",
    "print(f'{q = }, {r = }, {J - q = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e255c-e80f-473e-94e3-e0d4a8d9ab7a",
   "metadata": {},
   "source": [
    "We see that $J$ was obtained by rounding up, and that $J 2^{-N}$ is slightly larger than $x = 0.1$. More precisely, \n",
    "\n",
    "$$\n",
    "x 2^N = \\frac{2^N}{10} = q + \\frac{r}{10} = J-1 + \\frac{8}{10} = J - \\frac 1 5,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "J 2^{-N} - x = \\frac 1 5 2^{-N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895562-fbdd-4ea9-b6e8-91815d0de215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rounding error is\n",
    "err = 1/5 * 2**(-N)\n",
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dcd38-722f-4e8d-9581-463790af3bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Limits of floating-points numbers\n",
    "\n",
    "The limits of floating-point numbers are summarized in NumPy as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b9d2b-1c99-4d98-896e-d3699ba27df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.finfo(float))\n",
    "eps = np.finfo(float).eps\n",
    "print(eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f6822",
   "metadata": {},
   "source": [
    "We rarely want to think in binary, and most of the above limits are given in decimal. For example, `eps` is the [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon), that is, the difference between $1$ and the next smallest representable float larger than $1$. We write $x = 1 + \\epsilon$ for this number. \n",
    "Let's compute $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63594c-aa5c-4788-983b-a5d9b1496804",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Analogously to the above, we let\n",
    "$x = 1 + \\epsilon = J 2^{-N}$.\n",
    "Then $N = 52 - n$ where $n$ is the largest integer satisfying $n \\le \\log_2(x)$.\n",
    "As $x = 1 + \\epsilon$ is close to $1$, $\\log_2(x)$ is close to $0$.\n",
    "Hence $n = 0$ and $N = 52$. Now we can solve for $J$,\n",
    "\n",
    "$$\n",
    "J = 2^N x = 2^N + \\epsilon 2^N.\n",
    "$$\n",
    "\n",
    "We want to find the smallest possible $\\epsilon > 0$, while $J$ is an integer. Hence $J = 2^N + 1$ and \n",
    "\n",
    "$$\n",
    "\\epsilon = 2^{-N} = 2^{-52}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0798c-c91e-4f27-963d-d0056013c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.finfo(float).eps == 2**(-52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf485278-60eb-4744-a1cc-c242da83f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's return to our starting point and check that \n",
    "# 0.1 + 0.1 + 0.1 == 0.3\n",
    "# up to floating-point precision\n",
    "eps = np.finfo(float).eps\n",
    "np.abs(0.1 + 0.1 + 0.1 - 0.3) < eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59ce70-6be8-479e-a300-62b84808da78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Rounding error\n",
    "\n",
    "Let $x > 0$ be a real number, and let $J 2^{-N}$ be its best floating-point approximation. (In the case that there are two equally good approximations, we simply choose one of them.) We write again $n$ for the largest integer satisfying $n \\le \\log_2(x)$. Then $N = 52- n$ and\n",
    "\n",
    "$$\n",
    "|x 2^N - J| \\le \\frac 12.\n",
    "$$\n",
    "\n",
    "Therefore, using $2^n \\le 2^{\\log_2(x)} = x$,\n",
    "\n",
    "$$\n",
    "|x - J 2^{-N}| \\le \\frac 12 2^{-N} = \\frac 12 2^{-52} 2^n \\le \\frac \\epsilon 2 x,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is the machine epsilon. In other words, the _relative rounding error_,\n",
    "\n",
    "$$\n",
    "\\frac{|x - J 2^{-N}|}{x},\n",
    "$$\n",
    "\n",
    "is at most half of the machine epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503abd1-8c18-41c4-890d-4ad8cd193d15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Floating-point operations\n",
    "\n",
    "Finite precision of floating-point numbers will affect the elementary operations $+,-,\\cdot,/$. \n",
    "\n",
    "For simplicity, we consider a model of floating-point numbers that are stored using 8 decimal digits\n",
    "\n",
    "$$\n",
    "\\mathbb F = \\{ \\pm J 10^{-N} : J, N \\in \\mathbb Z,\\ 10^7 \\le J < 10^8 \\}.\n",
    "$$\n",
    "\n",
    "Note that we don't restrict here the size of the exponent $N$. In practice,\n",
    "numbers with too small $-N$ will be rounded to zero, and too large $-N$ will cause an overflow error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fdd0941",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: too small and large numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9634b8-82a1-4f5a-80fa-583b592d7656",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# It is important to use 2.0, not 2, here.\n",
    "# With 2, the integer data type is used and it has different limits. \n",
    "print(2.0**(-1074)) \n",
    "print(2.0**(-1075))\n",
    "print(2.0**1023)\n",
    "try:\n",
    "    print(2.0**1024)\n",
    "except OverflowError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb79e4-5498-453a-b492-b07297ee5cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Addition is modelled by\n",
    "\n",
    "$$\n",
    "x \\oplus y = \\mathrm{fl}(x+y),\n",
    "$$\n",
    "\n",
    "where $\\mathrm{fl}$ rounds to a nearest number in the set $\\mathbb F$, and other floating-point operations are defined analogously. \n",
    "\n",
    "The relative error in floating-point operations, due to rounding, is at most half of the machine epsilon. When reasoning mathematically, this is sometimes called the fundamental axiom of floating point arithmetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c3966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fl(x, precision = 8):\n",
    "    '''Simulate decimal floating-point rounding with given precision'''\n",
    "    return float(np.format_float_positional(x, \n",
    "        precision = precision, fractional = False))\n",
    "    \n",
    "def oplus(x, y):\n",
    "    return fl(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042b7d6-1e88-4bc8-84b8-3acc0ef6c1bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: sum of three numbers\n",
    "\n",
    "Let us compute $(a \\oplus b) \\oplus c$ and $a \\oplus (b \\oplus c)$ for $a,b,c \\in \\mathbb F$ defined by\n",
    "\n",
    "$$\n",
    "a = 0.23371258 \\cdot 10^{-4}, \n",
    "\\quad \n",
    "b = 0.33678429 \\cdot 10^2,\n",
    "\\quad\n",
    "c = -0.33677811 \\cdot 10^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0fcae-d76d-40fb-bc09-8b8c30fd3287",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "a = 0.23371258e-4\n",
    "b = 0.33678429e2\n",
    "c = -0.33677811e2\n",
    "print(f'''\n",
    "(1)  {oplus(oplus(a, b), c) = } \n",
    "(2)  {oplus(a, oplus(b, c)) = }\n",
    "optimal = {fl(a + b + c)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405d12b-d2fd-4ee0-9397-c3c67a37631b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As $b$ is much larger than $a$, only the first 3 digits of $a$ contribute to $a \\oplus b$, the rest are lost in the rounding.\n",
    "\n",
    "The second ordering gives the optimal result (obtained by computing the two sums in high precision and rounding only the end result). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e580367",
   "metadata": {},
   "source": [
    "## Example: uneven sum\n",
    "\n",
    "Many shortcomings are accounted by good implementations and those should be used whenever possible. Also note that math is usually even better than computitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fa6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0 if i%10==0 else 10**(-5) for i in range(100)])\n",
    "\n",
    "# Naive sum\n",
    "S = 0\n",
    "for s in x:\n",
    "    S += s\n",
    "print(f'Sum is {S}')\n",
    "print(f'Numpy says {np.sum(x)}')\n",
    "print(f'Math says {10*1.0 + 90*10**(-5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefc7a99",
   "metadata": {},
   "source": [
    "## Example: continuous adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1.9999999999999*2.0**52\n",
    "print(f'{a = }')\n",
    "for i in range(1000):\n",
    "    if i%25 == 0:\n",
    "        print(f'{i = }')\n",
    "    b = a\n",
    "    a += 1.0\n",
    "    if a == b: # a + 1.0 should be bigger than b\n",
    "        print(f'Underflow for {a = } when {i = }')\n",
    "        break\n",
    "# When did the underflow happen?\n",
    "print(np.log2(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7d324",
   "metadata": {},
   "source": [
    "## Example: variance of small floats\n",
    "\n",
    "Consider a uniformly distributed random variable $X \\sim U(a,b)$. Given $n$ samples $x_i, i = 1,...,n$ the [variance](https://en.wikipedia.org/wiki/Variance) is given by\n",
    "$$\\text{Var}(X) = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\mu)^2 = \\frac{1}{n-1}\\Big(\\sum_{i=1}^n x_i^2 - n \\mu^2 \\Big),$$\n",
    "where $\\mu$ is the mean.\n",
    "\n",
    "If the mean $\\mu \\approx 0$ (say for small $a = -b$), then we can get underflow in the differences which leads to errors. However variance is shift-invariant:\n",
    "$$\\text{Var}(X) = \\text{Var}(X - K),$$\n",
    "meaning we can shift the values $x_i$ (and $\\mu$) such that we get better precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11944bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Var(X):\n",
    "    mu = np.mean(X)\n",
    "    n = len(X)\n",
    "    return (np.sum(X**2) - n*(mu**2))/(n-1)\n",
    "\n",
    "n = 1000\n",
    "a = 10**(-7)\n",
    "X = np.random.uniform(-a, a, n)\n",
    "print(f'K = 0: {Var(X)}')\n",
    "K = X[0] # Common choice\n",
    "print(f'K = x_0: {Var(X-K)}')\n",
    "K = 1\n",
    "print(f'K = 1: {Var(X - K)}')\n",
    "\n",
    "# Implementation in numpy\n",
    "print(f'numpy: {np.var(X, ddof=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62220bfe-79a9-4392-bdb1-3a211a79ba12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Catastrophic cancellation\n",
    "\n",
    "In numerical analysis, catastrophic cancellation is the phenomenon that subtracting good approximations to two nearby numbers may yield a very bad approximation to the difference of the original numbers.\n",
    "\n",
    "* Catastrophic cancellation may happen even if the difference is computed exactly\n",
    "* It is not a property of any particular kind of arithmetic like floating-point arithmetic \n",
    "* Rather, it is inherent to subtraction, when the inputs are approximations themselves\n",
    "\n",
    "However, rounding in floating-point operations may amplify the cancellation effect, as happened in case (1) in the above example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b066d778",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: subtraction of measured quantities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4125c8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This example is taken from [Catastrophic cancellation](https://en.wikipedia.org/wiki/Catastrophic_cancellation) Wikipedia article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bc792-ff15-4da1-999c-d67dd048bee1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Consider two rods of lengths $L_1 = 254.5\\,\\text{cm}$ and $L_2 = 253.5\\,\\text{cm}$. If you measure them with a ruler that is good only to the centimeter, you might get approximations $\\tilde L_1 = 255\\,\\text{cm}$ and $\\tilde L_2 = 253\\,\\text{cm}$. \n",
    "The approximations are in relative error by less than 1% of the true lengths, \n",
    "\n",
    "$$\n",
    "\\frac{|L_1 - \\tilde L_1|}{|L_1|} < 1\\%.\n",
    "$$\n",
    "\n",
    "However, if you subtract the approximate lengths, you will get \n",
    "\n",
    "$$\n",
    "\\tilde L_1 - \\tilde L_2 = 255\\,\\text{cm} - 253\\,\\text{cm} = 2\\,\\text{cm},\n",
    "$$\n",
    "\n",
    "even though the true difference between the lengths is \n",
    "\n",
    "$$\n",
    "L_1 - L_2 = 254.5\\,\\text{cm} - 253.5\\,\\text{cm} = 1\\,\\text{cm}.\n",
    "$$\n",
    "\n",
    "The difference of the approximations is in relative error by 100% of the true difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141e532-5111-481e-8c26-8fefc05b1ada",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: rearranging computations\n",
    "\n",
    "Sometimes it is possible to arrange the operations so that cancellation due to subtraction does not occur. Consider computing $e^x$ via truncation of its Taylor series\n",
    "\n",
    "$$\n",
    "e^x \\approx 1 + x + \\frac{x^2}{2!} + \\dots \\frac{x^n}{n!}.\n",
    "$$\n",
    "\n",
    "Let $x = -10$. We can either simply substitute to the above formula or first use\n",
    "\n",
    "$$\n",
    "e^{-10} = \\frac{1}{e^{10}}\n",
    "$$\n",
    "\n",
    "and then substitute $x = 10$ in the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73994c96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def exp_demo(x, n):\n",
    "    '''Compute e^x by using its Taylor series up to the nth term\n",
    "    \n",
    "        1 + x + x^2/2! + ... + x^n/n!\n",
    "    '''\n",
    "    out = 1\n",
    "    xn = 1\n",
    "    cn = 1\n",
    "    for i in range(1, n + 1):\n",
    "        xn *= x\n",
    "        cn /= i\n",
    "        out += cn * xn\n",
    "    return out\n",
    "\n",
    "def relative_error(y):\n",
    "    ytrue = np.exp(-10)\n",
    "    return np.abs(y - ytrue) / ytrue\n",
    "\n",
    "ns = range(1, 60)\n",
    "err_left = [relative_error(exp_demo(-10, n)) for n in ns]\n",
    "err_right = [relative_error(1 / exp_demo(10, n)) for n in ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe472ab-cfb9-4972-bb49-88869f6601dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(dpi = 150)\n",
    "plt.semilogy(ns, err_left, 'r')\n",
    "plt.semilogy(ns, err_right, 'b')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel('n');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c243d6",
   "metadata": {},
   "source": [
    "# Common problems\n",
    "By construction computing with floats can lead to truncation, cancellation, overflow and underflow. In particular the following things which are trivial or useful in mathematics become source of errors or problems in computations:\n",
    "- Cancellation: subtraction of two nearby floats leaves only the most insignificant digits, rest is set to zero and after changing the exponent we have a very erroneous result.\n",
    "    - In math it is common to state $a \\neq b$ so take $(a - b) / 2$ etc. This does not necessarily work.\n",
    "    - For example derivative: $\\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$ will by defitition lead to cancellation and the limit will get *worse* as $h$ gets smaller.\n",
    "\n",
    "- Converting back to integers: although floats can represent many integers perfectly, unexpected things can happen.\n",
    "    - For example: `int(21.0/7.0) = 3` but `int(0.21/0.07) = 2`.\n",
    "\n",
    "- Division with small number: in math one shouldn't divide by zero. In computations one shouldn't divide by a small number either. The latter is harder to check because *small* depends on the magnitude of the numerator. The end result may also be completely wrong but not overflow, whereas division by zero will usually lead to immediate error or `nan`.\n",
    "\n",
    "- Testing for equality: similarly it may be impossible to get exact result so what is good enough? How much tolerance should there be? What is numerical error, what is implementation error?\n",
    "\n",
    "- Hardware differences: computers have different types of floating-point units (FPUs). Some can do more than others, some can use more precision (bits). A tolerance which works in one computer might be too strict in another.\n",
    "\n",
    "- Error propagation: small and indifferent error in one place can propagate and get amplified as it gets passed through the algorithm. Individual steps may be okay but the combination gives wrong results.\n",
    "    - For example solving a differential equations vs. solving an equivalent integral equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9df661",
   "metadata": {},
   "source": [
    "# Cost in floating point operations (FLOPs)\n",
    "It is common to compute the cost of a numerical method or algorithm with respect to [FLOPs](https://en.wikipedia.org/wiki/FLOPS). There are different ways of determining what constitutes 1 flop.\n",
    "\n",
    "One way is to define it as single operation of a floating-point unit (FPU) which it can carry out in one cycle of the CPU clock. The basic operation is the *multiply - accumulate*\n",
    "$$ a \\leftarrow a + bc$$\n",
    "This means that computing $a + b$, $a \\times b$ and $a + b \\times c$ are all equally expensive.\n",
    "\n",
    "Modern processors can do more complicated things such as compute the dot product of short vectors (or their segments) in one cycle.\n",
    "\n",
    "The mathematics way is to assign a cost of 1 flop to any basic artihmetic operation $+, -, \\times , \\div , \\sqrt, \\exp$. This is obviously wrong. It also makes measuring the cost super easy.\n",
    "\n",
    "For example this example plot by Lincoln Atkinson from the blog post [\"A simple benchmark of various math operations\"](https://latkin.org/blog/2014/11/09/a-simple-benchmark-of-various-math-operations/) shows that the computational cost of different operations varies a lot. Note that it is normalized such that addition has cost of 1 unit.\n",
    "\n",
    "![FLOPchart1.png](FLOPchart1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65542a1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "lo_title": "Floating-point arithmetics",
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
